# 用PyTorch实现线性回归#

PyTorch Fashion(风格)

1、prepare dataset
2、design model using Class  # 目的是为了前向传播forward，即计算y hat(预测值)
3、Construct loss and optimizer (using PyTorch API) 其中，计算loss是为了进行反向传播，optimizer是为了更新梯度。
4、Training cycle (forward,backward,update)

代码说明：

1、Module实现了魔法函数__call__()，call()里面有一条语句是要调用forward()。因此新写的类中需要重写forward()覆盖掉父类中的forward()
2、call函数的另一个作用是可以直接在对象后面加()，例如实例化的model对象，和实例化的linear对象
3、本算法的forward体现是通过以下语句实现的：
```
y_pred = model(x_data)
```
由于魔法函数call的实现,model(x_data)将会调用model.forward(x_data)函数，model.forward(x_data)函数中的
```
y_pred = self.linear(x)
```
self.linear(x)也由于魔法函数call的实现将会调用torch.nn.Linear类中的forward，至此完成封装，也就是说forward最终是在torch.nn.Linear类中实现的，具体怎么实现，可以不用关心，大概就是y= wx + b。
关于魔法函数call在PyTorch中的应用的进一步解释：
###pytorch 之 __call__, __init__,forward###
要学习pytorch，一个前提是 知道python calss中的__call__和__init__方法.
简单的说就是:

 - __init__: 类的初始化函数，类似于c++的构造函数
 - __call___: 使得类对象具有类似函数的功能。

__init__比较好理解，现在主要看一下 __call__的功能示例：
```
class A():
    def __call__(self):
        print('i can be called like a function')
 
 
a = A()
a()

```
out:
>i can be called like a function
让我们在调用时传入参数如何？
```
class A():
    def __call__(self, param):
        
        print('i can called like a function')
        print('掺入参数的类型是：', type(param))
 
 
a = A()
 
 
a('i')

```
out:
>i can called like a function
掺入参数的类型是： <class ‘str’>
发现对象a的表现完全类似一个函数。

那当然也可以在__call__里调用其他的函数啊,
在__call__函数中调用forward函数，并且返回调用的结果
```
class A():
    def __call__(self, param):
        
        print('i can called like a function')
        print('传入参数的类型是：{}   值为： {}'.format(type(param), param))
 
        res = self.forward(param)
        return res
 
    def forward(self, input_):
        print('forward 函数被调用了')
 
        print('in  forward, 传入参数类型是：{}  值为: {}'.format( type(input_), input_))
        return input_
 
a = A()
 
 
input_param = a('i')
print("对象a传入的参数是：", input_param)
 
 
```
out:
>i can called like a function
传入参数的类型是：<class ‘str’> 值为： i
forward 函数被调用了
in forward, 传入参数类型是：<class ‘str’> 值为: i
对象a传入的参数是： i

现在我们将初始化函数__init__也加上，来看一下：
在对象初始化时确定初始年龄，通过调用a(2)为对象年龄增加2岁，
```
class A():
    def __init__(self, init_age):
        super().__init__()
        print('我年龄是:',init_age)
        self.age = init_age
 
    def __call__(self, added_age):
        
 
        res = self.forward(added_age)
        return res
 
    def forward(self, input_):
        print('forward 函数被调用了')
        
        return input_ + self.age
print('对象初始化。。。。')
a = A(10)
 
 
input_param = a(2)
print("我现在的年龄是：", input_param)
 
 
```
out:
>对象初始化。。。。
我年龄是: 10
forward 函数被调用了
我现在的年龄是： 12

**pytorch主要也是按照__call__, __init__,forward三个函数实现网络层之间的架构的**
###pytorch系列nn.Modlue中call的进一步解释###
先看一个列子：
```
import torch
from torch import nn

m = nn.Linear(20, 30)
input = torch.randn(128, 20)
output = m(input)

output.size()

```
out:
>torch.Size([128, 30])
刚开始看这份代码是有点迷惑的，m是类对象，而直接像函数一样调用m，m(input)
**重点：**

 -** nn.Module 是所有神经网络单元（neural network modules）的基类**
 - pytorch在nn.Module中，**实现了__call__方法，而在__call__方法中调用了forward函数。**
经过以上两点。上述代码就不难理解。
接下来看一下源码：
![call][1]
再来看一下nn.Linear
[nn.linear][2]
主要看一下forward函数：
![nn][3]
返回的是：
***input∗weight+bias***
的线性函数

此时再看一下这一份代码：
```
import torch
from torch import nn

m = nn.Linear(20, 30)
input = torch.randn(128, 20)
output = m(input)

output.size()

```
首先创建类对象m，然后通过m(input)实际上调用__call__(input)，然后__call__(input)调用
forward()函数，最后返回计算结果为：
[128,20]×[20,30]=[128,30]
所以自己创建多层神经网络模块时，只需要在实现__init__和forward即可.

接下来看一个简单的三层神经网络的例子：
```
# define three layers
class simpleNet(nn.Module):

    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):
        super().__init__()
        self.layer1 = nn.Linear(in_dim, n_hidden_1)
        self.layer2 = nn.Linear(n_hidden_1, n_hidden_2)
        self.layer3 = nn.Linear(n_hidden_2, out_dim)

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)

        return x


```
以下为各层神经元个数：
输入： in_dim
第一层： n_hidden_1
第二层：n_hidden_2
第三层（输出层）：out_dim
4、本算法的反向传播，计算梯度是通过以下语句实现的：
```
loss.backward() # 反向传播，计算梯度
```
5、本算法的参数(w,b)更新，是通过以下语句实现的：
```
optimizer.step() # update 参数，即更新w和b的值
```
6、 每一次epoch的训练过程，总结就是

①前向传播，求y hat （输入的预测值）

②根据y_hat和y_label(y_data)计算loss

③反向传播 backward (计算梯度)

④根据梯度，更新参数
7、本实例是批量数据处理，小伙伴们不要被optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)误导了，以为见了SGD就是随机梯度下降。要看传进来的数据是单个的还是批量的。这里的x_data是3个数据，是一个batch，调用的PyTorch API是 torch.optim.SGD，但这里的SGD不是随机梯度下降，而是批量梯度下降。也就是说，梯度下降算法使用的是随机梯度下降，还是批量梯度下降，还是mini-batch梯度下降，用的API都是 torch.optim.SGD。

8、torch.nn.MSELoss也跟torch.nn.Module有关，参与计算图的构建，torch.optim.SGD与torch.nn.Module无关，不参与构建计算图。
9、传送门 [torch.nn.Linear的pytorch文档][4]
```
import torch
# prepare dataset
# x,y是矩阵，3行1列 也就是说总共有3个数据，每个数据只有1个特征
x_data = torch.tensor([[1.0], [2.0], [3.0]])
y_data = torch.tensor([[2.0], [4.0], [6.0]])
 
#design model using class
"""
our model class should be inherit from nn.Module, which is base class for all neural network modules.
member methods __init__() and forward() have to be implemented
class nn.linear contain two member Tensors: weight and bias
class nn.Linear has implemented the magic method __call__(),which enable the instance of the class can
be called just like a function.Normally the forward() will be called 
"""
class LinearModel(torch.nn.Module):
    def __init__(self):
        super(LinearModel, self).__init__()
        # (1,1)是指输入x和输出y的特征维度，这里数据集中的x和y的特征都是1维的
        # 该线性层需要学习的参数是w和b  获取w/b的方式分别是~linear.weight/linear.bias
        self.linear = torch.nn.Linear(1, 1)
 
    def forward(self, x):
        y_pred = self.linear(x)
        return y_pred
 
model = LinearModel()
 
# construct loss and optimizer
# criterion = torch.nn.MSELoss(size_average = False)
criterion = torch.nn.MSELoss(reduction = 'sum')
optimizer = torch.optim.SGD(model.parameters(), lr = 0.01) # model.parameters()自动完成参数的初始化操作
 
# training cycle forward, backward, update
for epoch in range(100):
    y_pred = model(x_data) # forward:predict
    loss = criterion(y_pred, y_data) # forward: loss
    print(epoch, loss.item())
 
    optimizer.zero_grad() # the grad computer by .backward() will be accumulated. so before backward, remember set the grad to zero
    loss.backward() # backward: autograd，自动计算梯度
    optimizer.step() # update 参数，即更新w和b的值
 
print('w = ', model.linear.weight.item())
print('b = ', model.linear.bias.item())
 
x_test = torch.tensor([[4.0]])
y_test = model(x_test)
print('y_pred = ', y_test.data)
```


  [1]: https://cdn.acwing.com/media/article/image/2022/10/31/192601_eb0b769658-call%E6%BA%90%E7%A0%81.png
  [2]: https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html
  [3]: https://cdn.acwing.com/media/article/image/2022/10/31/192601_1ef0bd6b58-nn.png
  [4]: https://pytorch.org/docs/1.7.0/generated/torch.nn.Linear.html#torch.nn.Linear