# 逻辑斯蒂回归
## 分类问题

## 问题引入

在前篇中提到的耗时效益的问题，是回归问题，也就是输入和输出间又数值上的关系。

| x(hours) | y(points) |
| --- | --- |
| 1 | 2 |
| 2 | 4 |
| 3 | 6 |
| 4 | ? |

但如果，将yy的值记为是否合格，此问题变变换成一个（二）分类问题。

| x(hours) | y(pass or fail) |
| --- | --- |
| 1 | fail(0) |
| 2 | fail(0) |
| 3 | pass(1) |
| 4 | ? |

实际上是计算在4工时下是否合格的概率，即对概率的计算与比较，而非类别之间的数值比较。

分类问题中，实际上是对概率的计算与比较，而非类别之间的数值比较

下文中介绍常见的MNIST数据集和CiFAR-10数据集

## MNIST数据集

![MNIST数据集示图](https://tuchuang980615.oss-cn-beijing.aliyuncs.com/img/image-20210302145101809.png)

该数据集规格：

1.  Training_set： 60000
2.  Test_set： 10000
3.  Classes： 10

## CIFAR-10数据集

![CIFAR-10数据集示图](https://tuchuang980615.oss-cn-beijing.aliyuncs.com/img/image-20210302145319135.png)

该数据集规格：

1.  Training_set: 50000
2.  Test_set: 10000
3.  Classes: 10

## 逻辑回归

## 二分类问题

二分类问题是非0即1的问题，由于隐藏条件的限制

$P(\widehat y=1)+P(\widehat y=0) = 1$

对于二分类问题结果的预测，仅需要计算在0或1的条件下即可得到答案。

通常计算$P(\widehat y=1)$
## 逻辑函数

在原先的回归问题中，所利用的模型为

$\widehat y = \omega x+b$

此时的$\widehat y \in R$，但当问题为分类问题时，所求结果的值域应当发生改变，变为一个概率即$\widehat y \in [0,1]$
因此，需要引入逻辑函数（sigmod）来实现。

$\sigma(x) = \frac{1}{1+e^{-x}}$

**本函数原名为logistics函数，属于sigmod类函数**，由于其特性优异，代码中的sigmod函数就指的是本函数。其函数图像为

![sigmod函数图像](https://tuchuang980615.oss-cn-beijing.aliyuncs.com/img/image-20210302151314093.png)

特点：

1.  函数值在0到1之间变化明显（导数大）
2.  在趋近于0和1处函数逐渐平滑（导数小）
3.  函数为饱和函数[\[1\]](#fn1)
4.  单调增函数

>满足$\lim \limits_{x\rightarrow+\infty}f'(x)\;\rightarrow0$ 的函数为右饱和函数；满足$\lim \limits_{x\rightarrow-\infty}f'(x)\;\rightarrow0$的函数为左饱和函数。同时满足二者的为饱和函数。

除上述以外，还有其他类的sigmod函数。

![其他类sigmod函数图像](https://tuchuang980615.oss-cn-beijing.aliyuncs.com/img/image-20210302152008499.png)

## 模型变化

### 模型结构变化

![原线性回归模型变为二分类模型](https://tuchuang980615.oss-cn-beijing.aliyuncs.com/img/image-20210302153609035.png)

### Loss变化

原先是计算两个标量数值间的差距，也就是数轴上的距离。

现在为了计算两个概率之间的差异，需要利用到交叉熵的理论。

下文信息量等相关理解整理自[史丹利复合田老师的博客](https://blog.csdn.net/tsyccnh/article/details/79163834)

![Loss变化](https://tuchuang980615.oss-cn-beijing.aliyuncs.com/img/image-20210302153738384.png)

#### 信息量

一个越不可能发生的事件发生了，我们获取到的信息量就越大，越可能的发生的事件发生了，我们获取到的信息量也就越小。因此信息量的大小应当和事件发生的概率有关，且成反比。为了研究方便，下述研究都是基于XX是离散分布的情况，若为连续分布则同理。

定义事件$X=x_0$的信息量为

$I(x_0) = -log(P(x_0))$

由于$P(x_0) \in [0,1]$ 因此，本函数图像大致为下图，当概率越大，$I(x_0)$越小。

![-log(x)的图像](https://tuchuang980615.oss-cn-beijing.aliyuncs.com/img/image-20210302160630003.png)

#### 熵

有了信息量的定义，对于某个事件所可能出现的$n$种可能性，我们可以求得该事件信息量的期望
离散分布求函数的期望公式
$$
E(X) = \sum_{i=1}^nP(X_i)f(X_i)
$$
由此可知信息量的期望为
$$
H(X) = -\sum_{i=1}^nP(X_i)log(P(X_i))
$$
将$H(x)$记为熵
#### 相对熵（KL散度）

上述问题是对于同一事件的不同可能性，而针对**同一个随机变量的两个单独的概率分布**，即对于同一个时间的两种概率分布，我们可以使用相对熵来衡量其差异。

相对熵指的是，如果当前对同样一个问题，有P、Q两种描述，假设P为真实描述（真实概率），可以完美表述问题。Q为非真实描述（非真实概率），不能够完美表述问题，与P之间存在信息差，如果弥补上这个信息差，Q就可以对该问题进行完美的表述。

为了描述P和Q之间的这个信息差，引入KL散度
$$
D_{KL}(p||q) = \sum^n_{i=1} P(X_i)log(\frac{P(X_i)}{Q(X_i)})
$$
因此$D_{KL}$越小，意味着Q分布与P分布越接近。

#### 交叉熵

对于相对熵的公式，变形可得
$$
D_{KL}(p||q) = \sum^n_{i=1} P(X_i)log(P(X_i))
-\sum^n_{i=1} P(X_i)log(Q(X_i))\\
=-H(P(X))+[-\sum^n_{i=1} P(X_i)log(Q(X_i))]
$$
由于等式前一部分恰好为P的熵，记后半部分为P、Q的交叉熵
$$
H(P,Q) =-\sum^n_{i=1} P(X_i)log(Q(X_i))
$$
在实际计算的过程中，P的熵是不变的，因此计算用于描述两个分布之间差异性的相对熵的问题，可以简化成计算交叉熵，交叉熵越小，则KL散度也就越小，两个分布对问题的描述程度也就越接近。
综上所述，二分类模型中的Loss改用交叉熵计算，即BCE
$$
Loss = -(y log(\widehat y)+(1-y)log((1-\widehat y))
$$
在小批量损失函数的计算中
$$
Loss = -\frac{1}{N}\sum_{n=1}^N y_nlog(\widehat y_n) + (1-y_n)log(1-\widehat y_n)
$$
如下图所示，$\widehat y$与$y$越接近，BCE Loss越小

![求Mini-Batch Loss的示例](https://tuchuang980615.oss-cn-beijing.aliyuncs.com/img/image-20210302170126712.png)
## 代码部分
```
import torch.nn.functional as F
import torch

x_data = torch.Tensor([[1.0],[2.0],[3.0]])
y_data = torch.Tensor([[0.0],[0.0],[1.0]])

#改用LogisticRegressionModel 同样继承于Module
class LogisticRegressionModel(torch.nn.Module):
    def __init__(self):
        super(LogisticRegressionModel, self).__init__()
        self.linear = torch.nn.Linear(1,1)

    def forward(self, x):
        #对原先的linear结果进行sigmod激活
        y_pred = F.sigmoid(self.linear(x))
        return y_pred
model = LogisticRegressionModel()

#构造的criterion对象所接受的参数为（y',y） 改用BCE
criterion = torch.nn.BCELoss(size_average=False)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

for epoch in range(1000):
    y_pred = model(x_data)
    loss = criterion(y_pred,y_data)
    print(epoch,loss)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

print('w = ', model.linear.weight.item())
print('b = ', model.linear.bias.item())

x_test = torch.Tensor([[4.0]])
y_test = model(x_test)

print('y_pred = ',y_test.data)
```
![结果实例](https://tuchuang980615.oss-cn-beijing.aliyuncs.com/img/image-20210302171048180.png)
* * *
# 相关函数详解
在前篇中提到的耗时效益的问题，是回归问题，也就是输入和输出间又数值上的关系。
![截图1][1]
说明：
1、 逻辑斯蒂回归和线性模型的明显区别是在线性模型的后面，添加了激活函数(非线性变换)
2、分布的差异：KL散度，cross-entropy交叉熵
![截图2][2]
说明：预测与标签越接近，BCE损失越小。
代码说明：
1、视频中代码F.sigmoid(self.linear(x))会引发warning，此处更改为torch.sigmoid(self.linear(x))
###torch.sigmoid() 与 torch.nn.Sigmoid() 对比###
1.torch.sigmoid()
![sig1][3]
2.torch.nn.Sigmoid()
![sig2][4]
只看文档，我没太看出二者的具体区别，通过以下可知得到结果自然相同，不过使用方式确实不同，我目前也没明白为啥：
![p1][5]
![p2][6]
![p3][7]
[torch.sigmoid()、torch.nn.Sigmoid()和torch.nn.functional.sigmoid()三者之间的区别][8]
2、BCELoss - Binary CrossEntropyLoss 
  BCELoss 是CrossEntropyLoss的一个特例，只用于二分类问题，而CrossEntropyLoss可以用于二分类，也可以用于多分类
  如果是二分类问题，建议BCELoss


<!--more-->


### 五分钟理解：BCELoss 和 BCEWithLogitsLoss的区别
由于中文版的挂了，这里先放英文版的
这里讲的是默认对一个batch里面的数据做二元交叉熵并且求平均。
![bce][9]
参数说明

 1. weight： 给每个batch元素的权重，一般没用
 2. size_average: 默认为True
 3. reduce: True/False 默认为True,对每个minibatch做
 4. reduction: 用的比较多的是这个，若用了2.3可能导致4失效。
![bce1][10]
**shape描述**
input与target都是一样的size
![bce2][11]
**官方实例**
```
>>> m = nn.Sigmoid()
>>> loss = nn.BCELoss()
>>> input = torch.randn(3, requires_grad=True)
>>> target = torch.empty(3).random_(2)
>>> output = loss(m(input), target)
>>> output.backward()

```
**运行结果**
```
import torch.nn as nn
m = nn.Sigmoid()
loss = nn.BCELoss()
input = torch.randn(3, requires_grad=True)
input
Out[21]: tensor([-0.6212, -0.9684,  0.6923], requires_grad=True)
target = torch.empty(3)
target
Out[23]: tensor([-1.5901e-30,  4.5907e-41,  0.0000e+00])
target = target.random_(2)
target
Out[25]: tensor([1., 0., 1.])
output = loss(m(input), target)
output
Out[27]: tensor(0.5929, grad_fn=<BinaryCrossEntropyBackward>)

```
### torch.empty()和torch.Tensor.random_()的使用举例
![te][12]
函数使用说明:
```
torch.empty(*size, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False)
根据给定形状和类型,返回一个包含未经初始化数据的张量.
```
函数使用说明:
```
random_(from=0, to=None, *, generator=None) → Tensor
用一个离散均匀分布[from, to - 1]来填充当前自身张量.

```
代码实验展示:
```
Microsoft Windows [版本 10.0.18363.1256]
(c) 2019 Microsoft Corporation。保留所有权利。

C:\Users\chenxuqi>conda activate ssd4pytorch1_2_0

(ssd4pytorch1_2_0) C:\Users\chenxuqi>python
Python 3.7.7 (default, May  6 2020, 11:45:54) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32
Type "help", "copyright", "credits" or "license" for more information.
>>> import torch
>>> input = torch.empty(2, 3)
>>> input
tensor([[0., 0., 0.],
        [0., 0., 0.]])
>>> input = torch.empty(2, 3)
>>> input
tensor([[0., 0., 0.],
        [0., 0., 0.]])
>>> torch.empty(2, 3)
tensor([[0., 0., 0.],
        [0., 0., 0.]])
>>> torch.empty(2, 3)
tensor([[0.0000e+00, 0.0000e+00, 8.4078e-45],
        [0.0000e+00, 1.4013e-45, 0.0000e+00]])
>>> torch.empty(2, 3)
tensor([[0., 0., 0.],
        [0., 0., 0.]])
>>> torch.empty(2, 3)
tensor([[0.0000e+00, 0.0000e+00, 8.4078e-45],
        [0.0000e+00, 1.4013e-45, 0.0000e+00]])
>>>
>>>
>>>
>>>
>>> torch.manual_seed(seed=20200910)
<torch._C.Generator object at 0x00000211B490D330>
>>>
>>> output = input.random_(0, to=10)
>>> output
tensor([[3., 8., 4.],
        [7., 8., 5.]])
>>> input
tensor([[3., 8., 4.],
        [7., 8., 5.]])
>>>
>>> input.random_(0, to=10)
tensor([[0., 8., 8.],
        [1., 6., 4.]])
>>> input.random_(0, to=10)
tensor([[2., 7., 9.],
        [0., 5., 0.]])
>>> input.random_(0, to=10)
tensor([[9., 9., 2.],
        [0., 4., 2.]])
>>>
>>>
>>>

```


<!--more-->

```
import torch
# import torch.nn.functional as F
 
# prepare dataset
x_data = torch.Tensor([[1.0], [2.0], [3.0]])
y_data = torch.Tensor([[0], [0], [1]])
 
#design model using class
class LogisticRegressionModel(torch.nn.Module):
    def __init__(self):
        super(LogisticRegressionModel, self).__init__()
        self.linear = torch.nn.Linear(1,1)
 
    def forward(self, x):
        # y_pred = F.sigmoid(self.linear(x))
        y_pred = torch.sigmoid(self.linear(x))
        return y_pred
model = LogisticRegressionModel()
 
# construct loss and optimizer
# 默认情况下，loss会基于element平均，如果size_average=False的话，loss会被累加。
criterion = torch.nn.BCELoss(size_average = False) 
optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)
 
# training cycle forward, backward, update
for epoch in range(1000):
    y_pred = model(x_data)
    loss = criterion(y_pred, y_data)
    print(epoch, loss.item())
 
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
 
print('w = ', model.linear.weight.item())
print('b = ', model.linear.bias.item())
 
x_test = torch.Tensor([[4.0]])
y_test = model(x_test)
print('y_pred = ', y_test.data)

```
其他：

关于BCE loss写了几行代码，帮助理解。 

target 中的数据需要是浮点型
```
import math
import torch
pred = torch.tensor([[-0.2],[0.2],[0.8]])
target = torch.tensor([[0.0],[0.0],[1.0]])
 
sigmoid = torch.nn.Sigmoid()
pred_s = sigmoid(pred)
print(pred_s)
"""
pred_s 输出tensor([[0.4502],[0.5498],[0.6900]])
0*math.log(0.4502)+1*math.log(1-0.4502)
0*math.log(0.5498)+1*math.log(1-0.5498)
1*math.log(0.6900) + 0*log(1-0.6900)
"""
result = 0
i=0
for label in target:
    if label.item() == 0:
        result +=  math.log(1-pred_s[i].item())
    else:
        result += math.log(pred_s[i].item())
    i+=1
result /= 3
print("bce：", -result)
loss = torch.nn.BCELoss()
print('BCELoss:',loss(pred_s,target).item())

```
  [1]: https://cdn.acwing.com/media/article/image/2022/10/31/192601_cfdda30f58-%E6%88%AA%E5%9B%BE1.png
  [2]: https://cdn.acwing.com/media/article/image/2022/10/31/192601_06e7267558-%E6%88%AA%E5%9B%BE2.png
  [3]: https://cdn.acwing.com/media/article/image/2022/10/31/192601_6cc8e4bc58-sig1.png
  [4]: https://cdn.acwing.com/media/article/image/2022/10/31/192601_8c03108d58-sig2.png
  [5]: https://cdn.acwing.com/media/article/image/2022/10/31/192601_b4f710e558-p1.png
  [6]: https://cdn.acwing.com/media/article/image/2022/10/31/192601_cdbcd55858-p2.png
  [7]: https://cdn.acwing.com/media/article/image/2022/10/31/192601_d80f1aee58-p3.png
  [8]: https://blog.csdn.net/weixin_42621901/article/details/107664771
  [9]: https://cdn.acwing.com/media/article/image/2022/10/31/192601_6e5affbd58-bce.jpg
  [10]: https://cdn.acwing.com/media/article/image/2022/10/31/192601_9f6d693258-bce1.jpg
  [11]: https://cdn.acwing.com/media/article/image/2022/10/31/192601_bd157ed058-bce2.jpg
  [12]: https://cdn.acwing.com/media/article/image/2022/10/31/192601_10609d5158-te.png