## 多分类问题
视频中截图

说明： 1、softmax的输入不需要再做非线性变换，也就是说softmax之前不再需要激活函数(relu)。softmax两个作用，如果在进行softmax前的input有负数，通过指数变换，得到正数。所有类的概率求和为1。

2、y的标签编码方式是one-hot。我对one-hot的理解是只有一位是1，其他位为0。(但是标签的one-hot编码是算法完成的，算法的输入仍为原始标签)

3、多分类问题，标签y的类型是LongTensor。比如说0-9分类问题，如果y = torch.LongTensor([3])，对应的one-hot是[0,0,0,1,0,0,0,0,0,0].(这里要注意，如果使用了one-hot，标签y的类型是LongTensor，糖尿病数据集中的target的类型是FloatTensor)

4、CrossEntropyLoss <==> LogSoftmax + NLLLoss。也就是说使用CrossEntropyLoss最后一层(线性层)是不需要做其他变化的；使用NLLLoss之前，需要对最后一层(线性层)先进行SoftMax处理，再进行log操作。
![d1][1]
![d2][2]
![d3][3]
![d4][4]
代码说明：
1、第8讲 from torch.utils.data import Dataset，第9讲 from torchvision import datasets。该datasets里面init，getitem,len魔法函数已实现。

2、torch.max的返回值有两个，第一个是每一行的最大值是多少，第二个是每一行最大值的下标(索引)是多少。

3、全连接神经网络

4、torch.no_grad()   Python中with的用法
### With语句是什么？
有一些任务，可能事先需要设置，事后做清理工作。对于这种场景，Python的with语句提供了一种非常方便的处理方式。其中一个很好的例子是文件处理，你需要获取一个文件句柄，从文件中读取数据，然后关闭文件句柄。
如果不用with语句，代码如下：
```
file = open("/tmp/foo.txt")
data = file.read()
file.close()

```
这里有两个问题。一是可能忘记关闭文件句柄；二是文件读取数据发生异常，没有进行任何处理。下面是处理异常的加强版本：
```
file = open("/tmp/foo.txt")
try:
    data = file.read()
finally:
    file.close()

```
这段代码运行良好，但是太冗长。这时候with便体现出了优势。 除了有更优雅的语法，with还可以很好的处理上下文环境产生的异常。下面是with版本的代码：
```
with open("/tmp/foo.txt") as file:
    data = file.read()
```
### with是如何工作的？
基本思想是:with所求值的对象必须有一个enter()方法，一个exit()方法。

紧跟with**后面的语句被求值后，返回对象的**__enter__()方法被调用，这个方法的返回值将被赋值给as后面的变量。当with后面的代码块全部被执行完之后，将调用前面返回对象的exit()方法。

下面是一个例子
```
######################
########with()##########
######################
class Sample:
    def __enter__(self):
        print("in __enter__")

        return "Foo"

    def __exit__(self, exc_type, exc_val, exc_tb):
                    #exc_type：　错误的类型 
                    #exc_val：　错误类型对应的值 
                    #exc_tb：　代码中错误发生的位置 
        print("in __exit__")

def get_sample():
    return Sample()
with get_sample() as sample:
    print("Sample: " ,sample)

```
运行代码，输出如下
![with1][5]
分析运行过程:

1.进入这段程序,首先创建Sample类,完成它的两个成员函数enter ()、exit()的定义,然后顺序向下定义get_sample()函数.
2.进入with语句,调用get_sample()函数,返回一个Sample()类的对象,此时就需要进入Sample()类中,可以看到
>1. __enter__()方法先被执行
>2. __enter__()方法返回的值 - 这个例子中是"Foo"，赋值给变量'sample'
>3. 执行with中的代码块，打印变量"sample",其值当前为 "Foo"
>4. 最后__exit__()方法被调用


5、代码中"_"的说明 
[Python中各种下划线的操作][6]
6、torch.max( )的用法  torch.max( )使用讲解
torch.max(input) → Tensor
返回输入tensor中所有元素的最大值
```
a = torch.randn(1, 3)
>>0.4729 -0.2266 -0.2085
 
torch.max(a)
>>0.4729

```
torch.max(input, dim, keepdim=False, out=None) -> (Tensor, LongTensor)

按维度dim 返回最大值

torch.max)(a,0) 返回每一列中最大值的那个元素，且返回索引（返回最大元素在这一列的行索引）
```
a = torch.randn(3,3)
>>
0.2252 -0.0901  0.5663
-0.4694  0.8073  1.3596
 0.1073 -0.7757 -0.8649
 
torch.max(a,0)
>>
(
 0.2252
 0.8073
 1.3596
[torch.FloatTensor of size 3]
, 
 0
 1
 1
[torch.LongTensor of size 3]
)

```
torch.max(a,1) 返回每一行中最大值的那个元素，且返回其索引（返回最大元素在这一行的列索引）
```
a = torch.randn(3,3)
>>
0.2252 -0.0901  0.5663
-0.4694  0.8073  1.3596
 0.1073 -0.7757 -0.8649
 
torch.max(a,1)
>>
(
 0.5663
 1.3596
 0.1073
[torch.FloatTensor of size 3]
, 
 2
 2
 0
[torch.LongTensor of size 3]
)

```
torch.max()[0]， 只返回最大值的每个数

troch.max()[1]， 只返回最大值的每个索引

torch.max()[1].data 只返回variable中的数据部分（去掉Variable containing:）

torch.max()[1].data.numpy() 把数据转化成numpy ndarry

torch.max()[1].data.numpy().squeeze() 把数据条目中维度为1 的删除掉

torch.max(tensor1,tensor2) element-wise 比较tensor1 和tensor2 中的元素，返回较大的那个值
在分类问题中，通常需要使用max()函数对softmax函数的输出值进行操作，求出预测值索引。下面讲解一下torch.max()函数的输入及输出值都是什么。
 - torch.max(input, dim) 函数
*output = torch.max(input, dim)*
>输入
 - input是softmax函数输出的一个tensor
 - dim是max函数索引的维度0/1，0是每列的最大值，1是每行的最大值
>输出
 - 函数会返回两个tensor，第一个tensor是每行的最大值，softmax的输出中最大的是1，所以第一个tensor是全1的tensor；第二个tensor是每行最大值的索引。
我们通过一个实例可以更容易理解这个函数的用法。
```
import torch
a = torch.tensor([[1,5,62,54], [2,6,2,6], [2,65,2,6]])
print(a)

```
输出：
```
tensor([[ 1,  5, 62, 54],
        [ 2,  6,  2,  6],
        [ 2, 65,  2,  6]])

```
索引每行的最大值：
```
torch.max(a, 1)
```
输出：
```
torch.return_types.max(
values=tensor([62,  6, 65]),
indices=tensor([2, 3, 1]))

```
在计算准确率时第一个tensor values是不需要的，所以我们只需提取第二个tensor，并将tensor格式的数据转换成array格式。
```
torch.max(a, 1)[1].numpy()

```
输出：
```
array([2, 3, 1], dtype=int64)
```
*注：在有的地方我们会看到torch.max(a, 1).data.numpy()的写法，这是因为在早期的pytorch的版本中，variable变量和tenosr是不一样的数据格式，variable可以进行反向传播，tensor不可以，需要将variable转变成tensor再转变成numpy。现在的版本已经将variable和tenosr合并，所以只用torch.max(a,1).numpy()就可以了。

2.准确率的计算
```
pred_y = torch.max(predict, 1)[1].numpy()
y_label = torch.max(label, 1)[1].data.numpy()
accuracy = (pred_y == y_label).sum() / len(y_label)

```
```
import torch
from torchvision import transforms
from torchvision import datasets
from torch.utils.data import DataLoader
import torch.nn.functional as F
import torch.optim as optim
 
# prepare dataset
 
batch_size = 64
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) # 归一化,均值和方差
 
train_dataset = datasets.MNIST(root='../dataset/mnist/', train=True, download=True, transform=transform)
train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)
test_dataset = datasets.MNIST(root='../dataset/mnist/', train=False, download=True, transform=transform)
test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)
 
# design model using class
 
 
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.l1 = torch.nn.Linear(784, 512)
        self.l2 = torch.nn.Linear(512, 256)
        self.l3 = torch.nn.Linear(256, 128)
        self.l4 = torch.nn.Linear(128, 64)
        self.l5 = torch.nn.Linear(64, 10)
 
    def forward(self, x):
        x = x.view(-1, 784)  # -1其实就是自动获取mini_batch
        x = F.relu(self.l1(x))
        x = F.relu(self.l2(x))
        x = F.relu(self.l3(x))
        x = F.relu(self.l4(x))
        return self.l5(x)  # 最后一层不做激活，不进行非线性变换
 
 
model = Net()
 
# construct loss and optimizer
criterion = torch.nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)
 
# training cycle forward, backward, update
 
 
def train(epoch):
    running_loss = 0.0
    for batch_idx, data in enumerate(train_loader, 0):
        # 获得一个批次的数据和标签
        inputs, target = data
        optimizer.zero_grad()
        # 获得模型预测结果(64, 10)
        outputs = model(inputs)
        # 交叉熵代价函数outputs(64,10),target（64）
        loss = criterion(outputs, target)
        loss.backward()
        optimizer.step()
 
        running_loss += loss.item()
        if batch_idx % 300 == 299:
            print('[%d, %5d] loss: %.3f' % (epoch+1, batch_idx+1, running_loss/300))
            running_loss = 0.0
 
 
def test():
    correct = 0
    total = 0
    with torch.no_grad():
        for data in test_loader:
            images, labels = data
            outputs = model(images)
            _, predicted = torch.max(outputs.data, dim=1) # dim = 1 列是第0个维度，行是第1个维度
            total += labels.size(0)
            correct += (predicted == labels).sum().item() # 张量之间的比较运算
    print('accuracy on test set: %d %% ' % (100*correct/total))
 
 
if __name__ == '__main__':
    for epoch in range(10):
        train(epoch)
        test()

```
  [1]: https://cdn.acwing.com/media/article/image/2022/10/31/192601_115af98858-d1.png
  [2]: https://cdn.acwing.com/media/article/image/2022/10/31/192601_2a056e9258-d2.png
  [3]: https://cdn.acwing.com/media/article/image/2022/10/31/192601_3ee5b76058-d3.png
  [4]: https://cdn.acwing.com/media/article/image/2022/10/31/192601_5bdc959158-d4.png
  [5]: https://cdn.acwing.com/media/article/image/2022/10/31/192601_e9bfce3158-with1.png
  [6]: https://zhuanlan.zhihu.com/p/105783765?utm_source=com.miui.notes