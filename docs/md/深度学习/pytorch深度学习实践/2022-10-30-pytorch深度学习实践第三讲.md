# 梯度下降
## 问题背景
### 穷举法
在《pytorch学习笔记003》中，所使用的思想基于穷举，即提前已经设定好参数的准确值在某个区间内并以某个步长进行穷举（np.arange(0.0,4.1,0.1)）。

这样的思想在多维的情况下，即多个参数的时候，会引起维度诅咒的情况，在一个N维曲面中找一个最低点。使得原问题变得不可解。基于这样的问题，需要进行改进。
### 分治法
大化小，小化无，先对整体进行分割采样，在相对最低点进行进一步采样，直到其步长与误差符合条件。

但分治法有两个缺点

 1. 容易只找到局部最优解，而不易找到一个全局最优解
 2. 如果需要分得更加细致，则计算量仍然巨大

由于以上问题的存在，引起了参数优化的问题，即求解使loss最小时的参数得值
![py6.png](https://cdn.acwing.com/media/article/image/2022/11/23/192601_587cfad96a-py6.png) 
##梯度下降算法
###梯度
梯度即导数变化最大的值，其方向为导数变化最大的方向。
![py7.png](https://cdn.acwing.com/media/article/image/2022/11/23/192601_8d9fc7bb6a-py7.png) 
若设▲x>0,则对于增函数，梯度为上升方向，对于减函数，梯度为下降方向。如此方向都不是离极值点渐进的方向，因此需要取梯度下降的方向即梯度的反方向作为变化方向。
### 梯度下降算法
以凸函数为例，对于当前所选择的$\omega$的值，显然并不是最低点。而对于将来要到达的全局的最低点，当前点只能向下取值，即向梯度下降方向变化。
>凸函数：二阶导数为正数的函数，图像上任意两点进行连线或切面，可以确认两点间的所有函数点均在连线或切面的上方或下方。

![当前w的取值点与最小值点](https://cdn.acwing.com/media/article/image/2022/11/23/192601_f2ce19c46a-py8.png) 
![py9.png](https://cdn.acwing.com/media/article/image/2022/11/23/192601_1ca4c50d6a-py9.png) 
![w按梯度下降进行更新](https://cdn.acwing.com/media/article/image/2022/11/23/192601_36eca2e86a-py10.png)
### 局限性

 1. 梯度下降算法容易进入局部最优解（非凸函数），但是实际问题中的局部最优点较少，或已经基本可以当成全局最优点
 2. 梯度下降算法容易陷入鞍点

>鞍点：在一个维度上看为极小值点，在其他维度上是极大值点

### 梯度公式
![py11.png](https://cdn.acwing.com/media/article/image/2022/11/23/192601_be8dd52b6a-py11.png) 


深度学习算法中，并没有过多的局部最优点。
## 代码及图示

```
import matplotlib.pyplot as plt
 
# prepare the training set
x_data = [1.0, 2.0, 3.0]
y_data = [2.0, 4.0, 6.0]
 
# initial guess of weight 
w = 1.0
 
# define the model linear model y = w*x
def forward(x):
    return x*w
 
#define the cost function MSE 
def cost(xs, ys):
    cost = 0
    for x, y in zip(xs,ys):
        y_pred = forward(x)
        cost += (y_pred - y)**2
    return cost / len(xs)
 
# define the gradient function  gd
def gradient(xs,ys):
    grad = 0
    for x, y in zip(xs,ys):
        grad += 2*x*(x*w - y)
    return grad / len(xs)
 
epoch_list = []
cost_list = []
print('predict (before training)', 4, forward(4))
for epoch in range(100):
    cost_val = cost(x_data, y_data)
    grad_val = gradient(x_data, y_data)
    w-= 0.01 * grad_val  # 0.01 learning rate
    print('epoch:', epoch, 'w=', w, 'loss=', cost_val)
    epoch_list.append(epoch)
    cost_list.append(cost_val)
 
print('predict (after training)', 4, forward(4))
plt.plot(epoch_list,cost_list)
plt.ylabel('cost')
plt.xlabel('epoch')
plt.show() 

```
![Cost与Epoch变化曲线](https://cdn.acwing.com/media/article/image/2022/11/23/192601_e74c04cc6a-py12.png) 
# 随机梯度下降法

随机梯度下降法在神经网络中被证明是有效的。效率较低(时间复杂度较高)，学习性能较好。

随机梯度下降法和梯度下降法的主要区别在于：

1、损失函数由cost()更改为loss()。cost是计算所有训练数据的损失，loss是计算一个训练函数的损失。对应于源代码则是少了两个for循环。

2、梯度函数gradient()由计算所有训练数据的梯度更改为计算一个训练数据的梯度。

3、本算法中的随机梯度主要是指，每次拿一个训练数据来训练，然后更新梯度参数。本算法中梯度总共更新100(epoch)x3 = 300次。梯度下降法中梯度总共更新100(epoch)次。
![py13.png](https://cdn.acwing.com/media/article/image/2022/11/23/192601_28ca73006a-py13.png) 
## 优势
有可能跨越鞍点（神经网络常用）
## 代码
```
import matplotlib.pyplot as plt
 
x_data = [1.0, 2.0, 3.0]
y_data = [2.0, 4.0, 6.0]
 
w = 1.0
 
def forward(x):
    return x*w
 
# calculate loss function
def loss(x, y):
    y_pred = forward(x)
    return (y_pred - y)**2
 
# define the gradient function  sgd
def gradient(x, y):
    return 2*x*(x*w - y)
 
epoch_list = []
loss_list = []
print('predict (before training)', 4, forward(4))
for epoch in range(100):
    for x,y in zip(x_data, y_data):
        grad = gradient(x,y)
        w = w - 0.01*grad    # update weight by every grad of sample of training set
        print("\tgrad:", x, y,grad)
        l = loss(x,y)
    print("progress:",epoch,"w=",w,"loss=",l)
    epoch_list.append(epoch)
    loss_list.append(l)
 
print('predict (after training)', 4, forward(4))
plt.plot(epoch_list,loss_list)
plt.ylabel('loss')
plt.xlabel('epoch')
plt.show() 

```
# 批量梯度下降（mini-batch）
在前面的阐述中，普通的梯度下降算法利用数据整体，不容易避免鞍点，算法性能上欠佳，但算法效率高。随机梯度下降需要利用每个的单个数据，虽然算法性能上良好，但计算过程环环相扣无法将样本抽离开并行运算，因此算法效率低，时间复杂度高。

综上可采取一种折中的方法，即批量梯度下降方法。

将若干个样本分为一组，记录一组的梯度用以代替随机梯度下降中的单个样本。

**该方法最为常用，也是默认接口**